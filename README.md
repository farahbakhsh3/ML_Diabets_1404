# Diabetes Prediction Using Machine Learning

This repository contains a **clean, modular, and reproducible** machine learning pipeline for predicting diabetes using clinical features.  
The project is designed following **ML best practices**, with proper cross-validation, pipeline-based preprocessing, and model tuning.

---

## ğŸ“Œ Project Overview

The goal of this project is to predict whether a patient has diabetes based on medical measurements such as glucose level, BMI, age, etc.

Key characteristics of this project:

- No data leakage (scaling is done inside pipelines)
- Proper cross-validation
- Multiple models compared fairly
- Best model selected using **F1-score**
- Hyperparameter tuning applied only to the winning model
- Final model saved for real-world usage

---

## ğŸ“Š Dataset

- **File:** `data/diabetes.csv`
- **Source:** Pima Indians Diabetes Dataset
- **Target column:** `Outcome`  
  - `0` â†’ No diabetes  
  - `1` â†’ Diabetes

### Features

| Feature | Description |
|------|-----------|
| Pregnancies | Number of pregnancies |
| Glucose | Plasma glucose concentration |
| BloodPressure | Diastolic blood pressure |
| SkinThickness | Triceps skin fold thickness |
| Insulin | 2-Hour serum insulin |
| BMI | Body mass index |
| DiabetesPedigreeFunction | Diabetes pedigree function |
| Age | Age in years |

---

## ğŸ“ Project Structure

```
ML_Diabets_1404/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ diabetes.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py      # Load dataset
â”‚   â”œâ”€â”€ models.py           # Model pipelines
â”‚   â”œâ”€â”€ evaluate.py         # Cross-validation metrics
â”‚   â”œâ”€â”€ tune.py             # Hyperparameter tuning
â”‚   â”œâ”€â”€ train.py            # Training & model selection
â”‚   â”œâ”€â”€ predict.py          # Inference script
â”‚   â””â”€â”€ visualization.py    # EDA & plots
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.joblib   # Final trained model
â”‚
â”œâ”€â”€ figures
â”‚   â””â”€â”€ plots/              # Auto-saved visualizations
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

Create a virtual environment (recommended):

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Exploratory Data Analysis (EDA) & Visualization

Before training, the dataset is explored and visualized to understand feature distributions, relationships, and potential outliers.  

**Key visualizations include:**

- Feature distributions (Histogram / KDE)
- Boxplots to detect outliers
- Correlation heatmap
- Pairplot of key features
- Class distribution  

**Sample visualizations (auto-saved in `plots/` folder):**

<img src="figures/plots/histograms.png" alt="Feature distributions (Histogram / KDE)" style="width:200px; height:auto;">

<img src="figures/plots/boxplots.png" alt="Boxplots to detect outliers" style="width:200px; height:auto;">

<img src="figures/plots/correlation_matrix.png" alt="Correlation Matrix" style="width:200px; height:auto;">

<img src="figures/plots/pairplot.png" alt="Pairplot of Key Features" style="width:200px; height:auto;">

<img src="figures/plots/class_distribution.png" alt="Class Distribution" style="width:200px; height:auto;">

---

### 2ï¸âƒ£ Train and select the best model

```bash
python src/train.py
```

This will:

- Compare multiple models using cross-validation
- Select the best model based on **F1-score**
- Apply hyperparameter tuning to the winning model
- Save the final model to `models/best_model.joblib`

---

### 3ï¸âƒ£ Make a prediction

```bash
python src/predict.py
```

The prediction script:

- Loads the trained model
- Accepts a sample input as a **pandas DataFrame**
- Outputs prediction and class probabilities

---

## ğŸ§© Handling Missing Values (Zero Imputation)

Some features (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) contain `0` values considered **missing**.

- Missing values are replaced using **median imputation** inside the pipeline.
- Prevents **data leakage** because the median is computed only from training data.
- Scaling is applied **after imputation** for models sensitive to feature magnitude.

---

## âš–ï¸ Scaling Strategy

- **Models sensitive to feature scales:** SVM, MLP, KNN, Naive Bayes â†’ `StandardScaler` applied  
- **Tree-based models:** DecisionTree, RandomForest â†’ Scaling not necessary  
- Ensures consistent preprocessing without bias or leakage.

---

## ğŸ§  Models Used

| Model | Scaling | Notes |
|-------|---------|-------|
| Naive Bayes | Yes | GaussianNB, scale applied due to feature variance |
| KNN | Yes | KNN requires scaling for distance calculation |
| SVM | Yes | Kernel-based model sensitive to scale, class_weight="balanced" |
| DecisionTree | No | Tree-based, scale not needed |
| RandomForest | No | Tree-based, scale not needed, class_weight="balanced" |
| MLP | Yes | Neural network sensitive to scale, hidden_layer_sizes=(64,32) |

---

## ğŸ“ˆ Evaluation Strategy

- **5-Fold Cross-Validation**
- Metrics: Accuracy, F1-score (primary), ROC-AUC
- F1-score is prioritized due to mild class imbalance

---

## ğŸ” Hyperparameter Tuning

- Implemented with `GridSearchCV`
- Applied only to the **best-performing model**
- Optimizes F1-score, prevents overfitting

---

## ğŸ›¡ï¸ Design Decisions

- Pipelines prevent data leakage
- StandardScaler applied selectively
- Feature names preserved during inference
- Modular, reusable code

---

## ğŸ“¦ Model Persistence

Final trained model:

```
models/best_model.joblib
```

- Ready for API, web app, or production inference

---

## ğŸ”® Future Improvements

- Handle class imbalance explicitly (SMOTE / class weights)
- Threshold tuning based on ROC curve
- REST API deployment (FastAPI)
- Model explainability (SHAP)

---

## ğŸ“œ License

GPL-3.0 License

---

## ğŸ‘¤ Author

Clean and defensible machine learning implementation prioritizing correctness over superficial results.

